# -*- coding: utf-8 -*-
"""PlantVillage Desease Classification using ResNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BUfjk7-UdThNqzLwBS8MiuOpgj33AQVP
"""

from google.colab import drive
drive.mount('/content/drive')

import tensorflow as tf
from tensorflow import keras
from keras import models, layers
from keras.applications import VGG19
from keras.models import Sequential
from keras.layers import Flatten, Dense, MaxPool2D, Dropout
from keras.optimizers import Adam
import matplotlib.pyplot as plt
import pandas as pd

IMG_SIZE = 224
BATCH_SIZE = 20
CHANNEL = 3
EPOCHS = 20

# IF BELOW CELL DOES'NT WORK USE THIS METHOD
# Replace 'folder_id' with the actual folder ID from the shared link
# folder_id = 'abcd1234efgh5678ijkl'

# # Construct the shared folder link
# shared_folder_link = f'/content/drive/MyDrive/{folder_id}'

# # Use the shared folder link to access the contents
# dataset_path = shared_folder_link

dataset_path = '/content/drive/MyDrive/PlantVillage'
# dataset_path = '/content/drive/shared-with-me/PlantVillage' #For shared data
dataset = tf.keras.preprocessing.image_dataset_from_directory(
    dataset_path,
    shuffle=True,
    image_size = (IMG_SIZE, IMG_SIZE),
    batch_size = BATCH_SIZE
)

classes = dataset.class_names
print(classes)
print(len(classes))

len(dataset)

for image_batch, label_batch in dataset.take(1):
  print(image_batch[0].shape)
  plt.imshow(image_batch[0].numpy().astype("uint8"))
  plt.title(classes[label_batch[0]])
  plt.axis("off")

plt.figure(figsize=(12, 9))

for image_batch, label_batch in dataset.take(1):
  for i in range(12):
    ax = plt.subplot(3, 4, i+1)
    plt.imshow(image_batch[i].numpy().astype("uint8"))
    plt.title(classes[label_batch[i]])
    plt.axis("off")

"""## Train Test Split"""

len(dataset)

# 80% --> training
# 20% --> 10% validation, 10% test

train_size = 0.8
len(dataset)*train_size

train_ds = dataset.take(72)
len(train_ds)

test_ds = dataset.skip(72)
len(test_ds)

val_size = 0.1
len(dataset)*val_size

test_ds = test_ds.skip(9)
len(test_ds)

def get_dataset_partitions_tf(ds, train_split = 0.8, val_split = 0.1, test_split = 0.1, shuffle = True, shuffle_size = 10000):
  ds_size = len(ds)

  if shuffle:
    ds = ds.shuffle(shuffle_size, seed = 12)

  train_size = int(train_split * ds_size)
  val_size = int(val_split * ds_size)

  train_ds = ds.take(train_size)

  val_ds = ds.skip(train_size).take(val_size)
  test_ds = ds.skip(train_size).skip(val_size)


  return train_ds, val_ds, test_ds

train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)

print("Length of training: ", len(train_ds))
print("Length of validation: ", len(val_ds))
print("Length of test: ", len(test_ds))

resize_and_rescalling = tf.keras.Sequential([
    layers.Rescaling(1.0/255),  # Rescale pixel values to [0, 1]
    layers.Resizing(IMG_SIZE, IMG_SIZE)
    ])

"""## Data Augmentation"""

data_augmentation = tf.keras.Sequential([
    layers.RandomFlip("horizontal_and_vertical"),  # Randomly flip images horizontally and vertically
    layers.RandomRotation(0.2)  # Randomly rotate images by up to 20 degrees
])

"""## Modeling ResNet50"""

from keras.applications import ResNet50

input_shape = (BATCH_SIZE, IMG_SIZE, IMG_SIZE, CHANNEL)
n_classes = 10

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, CHANNEL))

for layer in base_model.layers:
    layer.trainable = False

model = models.Sequential([
    resize_and_rescalling,
    data_augmentation,
    base_model,
    layers.GlobalAveragePooling2D(),  # Replace Flatten layer with GlobalAveragePooling2D
    layers.Dense(n_classes, activation="softmax")  # Output layer for classification
])

model.build(input_shape=(32, 224, 224, 3))

model.compile(
    optimizer="Adam",
    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    metrics = ['accuracy']
)

model.summary()

history = model.fit(
    train_ds,
    epochs = EPOCHS,
    batch_size = BATCH_SIZE,
    verbose = 1,
    validation_data = val_ds
)

history.history.keys()

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

"""## Visualizing Accuracy"""

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(range(EPOCHS), acc, label="Training Accuracy")
plt.plot(range(EPOCHS), val_acc, label="Validation Accuracy")
plt.legend(loc="lower right")
plt.title("Training and Vaidation Accuracy")

#For loss
plt.subplot(1, 2, 2)
plt.plot(range(EPOCHS), loss, label="Training Loss")
plt.plot(range(EPOCHS), val_loss, label="Validation Loss")
plt.legend(loc="lower right")
plt.title("Training and Vaidation Loss")
plt.show()

"""## Testing"""

import numpy as np

for images_batch, label_batch in test_ds.take(1):

  first_image = images_batch[0].numpy().astype("uint8")
  first_label = label_batch[0].numpy()

  print("First image to predict")
  plt.imshow(first_image)
  print("Actual Label: ", classes[first_label])

  batch_prediction = model.predict(image_batch)
  print("Predicted Label: ", classes[np.argmax(batch_prediction[0])])

def predict(model, img):
  img_array = tf.keras.preprocessing.image.img_to_array(images[i].numpy())
  img_array = tf.expand_dims(img_array, 0)

  predictions = model.predict(img_array)

  predicted_class = classes[np.argmax(predictions[0])]
  confidence = round(100 * (np.max(predictions[0])), 2)
  return predicted_class, confidence

plt.figure(figsize=(15, 15))

for images, labels in test_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i+1)
    plt.imshow(images[i].numpy().astype("uint8"))

    predicted_class, confidence = predict(model, images[i].numpy())
    actual_class = classes[labels[i]]

    plt.title(f"Actual: {actual_class},\n Predicted: {predicted_class},\n Confidence: {confidence}%")

    plt.axis("off")

"""## Saving Model Version"""

# Define your model version
model_version = 1  # You can set this based on your needs

# Save the model
model.save(f"modelResNet50_{model_version}")

# Download the saved model file
from google.colab import files
files.download(f"modelResNet50_{model_version}")

# Define your model version
model_version = 1  # You can set this based on your needs

# Save the model
model.save(f"modelResNet50_{model_version}")

# Compress the saved model directory into a zip file
import shutil
shutil.make_archive(f"modelResNet50_{model_version}", 'zip', f"modelResNet50_{model_version}")

# Download the zip file
from google.colab import files
files.download(f"modelResNet50_{model_version}.zip")

